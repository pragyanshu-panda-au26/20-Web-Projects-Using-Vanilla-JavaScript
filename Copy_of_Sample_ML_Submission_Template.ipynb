{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pragyanshu-panda-au26/20-Web-Projects-Using-Vanilla-JavaScript/blob/master/Copy_of_Sample_ML_Submission_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -**  - Pragyanshu\n",
        "##### **Team Member 2 -**\n",
        "##### **Team Member 3 -**\n",
        "##### **Team Member 4 -**"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary of EDA, Visualization, and K-Means Clustering on a Retail Database\n",
        "\n",
        "#### Introduction\n",
        "The retail database analysis aimed to uncover patterns and segment customers to enhance business strategies. The database contained transactional data including customer demographics, purchase history, product details, and other relevant metrics. The process involved Exploratory Data Analysis (EDA), data visualization, and the implementation of a K-Means clustering model.\n",
        "\n",
        "#### Exploratory Data Analysis (EDA)\n",
        "EDA was the first step to understand the structure, main characteristics, and potential anomalies in the data. Key activities included:\n",
        "\n",
        "1. **Data Cleaning**: Identified and handled missing values, outliers, and inconsistencies. This included removing duplicates, addressing null values through imputation or deletion, and standardizing data formats.\n",
        "\n",
        "2. **Descriptive Statistics**: Calculated mean, median, standard deviation, and range for numerical variables to summarize the central tendency and dispersion.\n",
        "\n",
        "3. **Customer Analysis**: Analyzed customer demographics such as age, gender, and geographic location. For instance, the majority of customers were between 30-40 years old, with a balanced gender distribution, predominantly from urban areas.\n",
        "\n",
        "4. **Product Analysis**: Assessed product categories, sales volume, and revenue generation. High revenue was concentrated in electronics and fashion categories.\n",
        "\n",
        "5. **Transaction Analysis**: Examined purchase frequency, average transaction value, and peak purchase times. Notably, weekends showed higher transaction volumes, and average transaction value peaked during festive seasons.\n",
        "\n",
        "#### Data Visualization\n",
        "Data visualization helped in presenting insights and patterns discovered during EDA:\n",
        "\n",
        "1. **Histograms and Density Plots**: Used for visualizing the distribution of numerical features such as age, transaction amounts, and product prices.\n",
        "\n",
        "2. **Bar Charts**: Illustrated categorical data like product categories, customer regions, and payment methods. For example, bar charts showed that electronics had the highest sales, followed by fashion.\n",
        "\n",
        "3. **Box Plots**: Employed to detect outliers and understand the spread of data, especially in transaction amounts and product prices.\n",
        "\n",
        "4. **Heatmaps**: Displayed correlation matrices to highlight relationships between variables. Strong correlations were observed between total purchase value and number of items bought.\n",
        "\n",
        "5. **Scatter Plots**: Visualized relationships between two continuous variables. Scatter plots of transaction value versus customer age provided insights into spending patterns across age groups.\n",
        "\n",
        "#### K-Means Clustering\n",
        "K-Means clustering was implemented to segment customers into distinct groups based on their purchasing behavior. The process involved:\n",
        "\n",
        "1. **Feature Selection**: Selected key features for clustering including total expenditure, purchase frequency, average transaction value, and recency (time since last purchase).\n",
        "\n",
        "2. **Standardization**: Standardized the data to ensure each feature contributed equally to the distance calculations used in clustering.\n",
        "\n",
        "3. **Determining Optimal Clusters**: Utilized the Elbow Method and Silhouette Analysis to determine the optimal number of clusters. The Elbow Method suggested 4 clusters, which was confirmed by silhouette scores indicating well-separated clusters.\n",
        "\n",
        "4. **Model Training**: Trained the K-Means model with 4 clusters, iterating to ensure stability and accuracy of the clusters.\n",
        "\n",
        "5. **Cluster Profiling**: Analyzed each cluster to understand the characteristics:\n",
        "   - **Cluster 1 (High Spenders)**: Customers with high total expenditure and frequent purchases.\n",
        "   - **Cluster 2 (Medium Spenders)**: Moderate expenditure and frequency, often purchasing mid-range products.\n",
        "   - **Cluster 3 (Low Spenders)**: Low expenditure and infrequent purchases, possibly budget-conscious.\n",
        "   - **Cluster 4 (Occasional Shoppers)**: Low frequency but high average transaction value, likely purchasing premium products occasionally.\n",
        "\n",
        "#### Insights and Recommendations\n",
        "The clustering analysis provided actionable insights:\n",
        "\n",
        "1. **Targeted Marketing**: Develop personalized marketing strategies for each cluster. High spenders could be targeted with loyalty programs, while low spenders might benefit from discounts and promotions.\n",
        "\n",
        "2. **Inventory Management**: Align stock levels with the preferences of different customer segments to optimize inventory turnover.\n",
        "\n",
        "3. **Customer Retention**: Focus on retaining high and medium spenders through engagement programs and enhancing customer experience.\n",
        "\n",
        "4. **Product Development**: Tailor product offerings based on the preferences and spending patterns of each cluster, introducing products that appeal to high-value clusters.\n",
        "\n",
        "#### Conclusion\n",
        "The EDA, visualization, and K-Means clustering provided a comprehensive understanding of customer behavior in the retail database. These insights enable more informed decision-making, targeted marketing efforts, and improved customer satisfaction, ultimately driving business growth."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[link]()"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write Problem Statement Here.**\n",
        "\n",
        "### Problem Statement\n",
        "\n",
        "This project addresses the following specific problems:\n",
        "1. **Data Understanding and Cleaning**: How can we preprocess the retail database to ensure the data is clean, consistent, and ready for analysis?\n",
        "2. **Insight Generation through EDA and Visualization**: What are the key characteristics and patterns in customer demographics, product categories, and purchasing behavior?\n",
        "3. **Customer Segmentation**: How can we effectively segment customers using a K-Means clustering model to identify distinct groups with similar purchasing behaviors?\n",
        "4. **Actionable Insights**: What actionable insights can be derived from the customer segments to improve marketing strategies, inventory management, and customer retention efforts?\n",
        "\n",
        "By solving these problems, the analysis will provide valuable insights into customer behavior, enabling the retail business to make data-driven decisions that enhance profitability and customer satisfaction.\n"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install matplotlib\n",
        "!pip install seaborn\n",
        "!pip install sklearn\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import datetime as dt\n",
        "import sklearn\n",
        "import pickle\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans"
      ],
      "metadata": {
        "id": "wJUEmPX3MT2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xgKYuYeKU2WC",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "dataset = pd.read_csv('/content/drive/MyDrive/Online Retail.xlsx - Online Retail.csv')\n",
        "dataset"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(\"Total Rows:\",dataset.shape[0])\n",
        "print(\"Total Column:\",dataset.shape[1])"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "dataset.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "dataset.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "dataset.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(dataset.isnull())"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "dataset.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    dataset['InvoiceDate'] = pd.to_datetime(dataset['InvoiceDate'])\n",
        "except KeyError:\n",
        "    print(\"Error: 'InvoiceDate' column not found.\")\n",
        "    print(\"Available columns:\", dataset.columns)\n",
        "    raise"
      ],
      "metadata": {
        "id": "0Bv92HhqSyk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "dataset.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.InvoiceNo: Unique identifier for each transaction.\n",
        "\n",
        "2.StockCode: Unique identifier for each product.\n",
        "\n",
        "3.Description: Textual description of the product.\n",
        "\n",
        "4.Quantity: Number of units of the product purchased.\n",
        "\n",
        "5.InvoiceDate: Date and time when the transaction was generated.\n",
        "\n",
        "6.UnitPrice: Price per unit of the product.\n",
        "\n",
        "7.CustomerID: Unique identifier for the customer.\n",
        "\n",
        "8.Country: Country where the customer resides or the transaction took place.\n",
        "\n",
        "9.Sales: Total sales value for each transaction line (Quantity * UnitPrice).\n",
        "\n",
        "10.Month: Month extracted from the InvoiceDate.\n",
        "\n",
        "11.DayOfWeek: Day of the week extracted from the InvoiceDate.\n",
        "\n",
        "12.Hour: Hour of the day extracted from the InvoiceDate."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in dataset.columns.tolist():\n",
        "  print(\"No. of unique values in \",i,\"is\",dataset[i].nunique(),\".\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "dataset.drop_duplicates(inplace=True)\n",
        "dataset.dropna(inplace=True)\n",
        "dataset.isnull().sum()"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I checked for duplicates and dropped duplicates abd sane fir the null values"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Get the top 10 most frequent descriptions\n",
        "top_10_descriptions = dataset['Description'].value_counts().head(10)\n",
        "\n",
        "# Create a bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=top_10_descriptions.values, y=top_10_descriptions.index)\n",
        "plt.xlabel('Number of Items Sold')\n",
        "plt.ylabel('Description')\n",
        "plt.title('Top 10 Most Frequent Descriptions')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "A horizontal bar chart was selected to easily compare the count of each product description while accommodating long category names."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It highlights the top 10 most frequent product descriptions, are White Hanging Heart T-light Holder, Regency Cakeststand 3Tier,Jumbo Bag Red Retrospot, Party Bunting,Lunch Bag Red Retrospot, Assorted Colour Bird Ornament, Set of 3 Cake Tins Pantry Design, Pack of 72 Retrospot Cake cases, Lunch Bag Black Skull, Natural Slate Heart Chalkboard"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Yes, by informing stock management, marketing tactics, and product development based on customer demand, potentially leading to increased sales and customer satisfaction."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "#Top Selling Products\n",
        "top_products = dataset.groupby('Description')['Quantity'].sum().sort_values(ascending=False).head(10)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=top_products.values, y=top_products.index)\n",
        "plt.title('Top Selling Products')\n",
        "plt.xlabel('Quantity Sold')\n",
        "plt.ylabel('Product')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart was chosen to visually represent the sales count of each product, allowing for easy identification of the top selling items."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It highlights the top 3 most purchased product are World war 2 Gliders Asstd Designs, Jumnbo Bag Red Retrospot, Assorted Colour Bird Ornament"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, understanding the top selling products can optimize stock levels, guide marketing efforts, and enhance product offerings, ultimately driving revenue and customer satisfaction."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "#Sales by Country\n",
        "dataset['Sales'] = dataset['Quantity'] * dataset['UnitPrice']\n",
        "sales_by_country = dataset.groupby('Country')['Sales'].sum().sort_values(ascending=False).head(10)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=sales_by_country.values, y=sales_by_country.index)\n",
        "plt.title('Sales by Country')\n",
        "plt.xlabel('Sales')\n",
        "plt.ylabel('Country')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart was chosen to visually compare the total sales value across different countries, allowing for easy identification of top-performing markets."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart highlights the range and frequency of different order values. It shows the most of the order are coming from United Kingdom, and leats is Sweden"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Yes, understanding sales distribution by country can inform marketing efforts, pricing strategies, and resource allocation to maximize revenue and market penetration, leading to business growth and profitability."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# Number of Orders per Customer\n",
        "orders_per_customer = dataset.groupby('CustomerID')['InvoiceNo'].nunique()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(orders_per_customer, bins=30, kde=True)\n",
        "plt.title('Number of Orders per Customer')\n",
        "plt.xlabel('Number of Orders')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram or bar chart is ideal for visualizing the distribution of order counts among customers, allowing for easy identification of frequent buyers and potential outliers.\n"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i get to know that average number of order per cutomer is 0-200"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, understanding customer order frequency can inform customer retention efforts, personalized marketing campaigns, and loyalty programs, ultimately leading to increased customer satisfaction and repeat business."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Distribution of Order Value\n",
        "order_value = dataset.groupby('InvoiceNo')['Sales'].sum()\n",
        "average_order_value = order_value.mean()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(order_value, bins=30, kde=True)\n",
        "plt.axvline(average_order_value, color='r', linestyle='--')\n",
        "plt.title('Distribution of Order Value')\n",
        "plt.xlabel('Order Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "print(f'Average Order Value: ${average_order_value:.2f}')"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram or density plot effectively visualizes the distribution of order values, allowing for easy identification of common purchase amounts and outliers.\n"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals the spread and concentration of order values, aiding in understanding customer spending patterns, identifying high-value transactions, and detecting anomalies."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, understanding the distribution of order values can inform pricing strategies, product bundling decisions, and targeted marketing efforts to optimize revenue generation and customer satisfaction."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "# Quantity Ordered by Customer\n",
        "quantity_per_customer = dataset.groupby('CustomerID')['Quantity'].sum().sort_values(ascending=False).head(10)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=quantity_per_customer.values, y=quantity_per_customer.index)\n",
        "plt.title('Top 10 Customers by Quantity Ordered')\n",
        "plt.xlabel('Quantity Ordered')\n",
        "plt.ylabel('Customer ID')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram or bar chart effectively visualizes the distribution of quantities purchased, allowing for easy identification of common purchase quantities and outliers."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals the distribution of order quantities per customer, helping identify common purchasing patterns, segment customer groups based on buying habits, and target marketing efforts accordingly."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, understanding the distribution of order quantities can inform inventory management, product bundling strategies, and personalized marketing campaigns, ultimately leading to increased customer satisfaction and sales.\n"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# Average Order Value by Country\n",
        "avg_order_value_by_country = dataset.groupby('Country')['Sales'].mean().sort_values(ascending=False).head(10)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=avg_order_value_by_country.values, y=avg_order_value_by_country.index)\n",
        "plt.title('Average Order Value by Country')\n",
        "plt.xlabel('Average Order Value')\n",
        "plt.ylabel('Country')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart effectively compares the average order value across different countries, making it easy to identify which countries contribute the most to revenue"
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I get to know that Netherlands contribute the most in revenue on the other hand EIRE is the least"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, understanding the average order value by country can inform resource allocation, marketing budget allocation, and pricing adjustments to maximize revenue and profitability in each market."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "95InlQMwRMKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "# Create a heatmap using seaborn\n",
        "\n",
        "\n",
        "dataset['Sales'] = dataset['Quantity'] * dataset['UnitPrice']\n",
        "# Extract additional date features\n",
        "dataset['Month'] = dataset['InvoiceDate'].dt.month\n",
        "dataset['DayOfWeek'] = dataset['InvoiceDate'].dt.dayofweek\n",
        "dataset['Hour'] = dataset['InvoiceDate'].dt.hour\n",
        "\n",
        "# Select numerical columns for correlation analysis\n",
        "numerical_columns = ['Quantity', 'UnitPrice', 'Sales', 'Month', 'DayOfWeek', 'Hour']\n",
        "\n",
        "# Compute the correlation matrix\n",
        "correlation_matrix = dataset[numerical_columns].corr()\n",
        "\n",
        "# Create a heatmap using seaborn\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because Heatmap is the best among showing relations between two variable."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I get to know that Sales and quantity is diretly related where as sales and unit price is inversly related"
      ],
      "metadata": {
        "id": "dN1dScT_9tpQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select numerical columns for pair plot analysis\n",
        "# Select numerical columns for pair plot analysis\n",
        "numerical_columns = ['Quantity', 'UnitPrice', 'Sales', 'Month', 'DayOfWeek', 'Hour']\n",
        "\n",
        "# Sample the data (e.g., 10% of the original data)\n",
        "sampled_dataset = dataset[numerical_columns].sample(frac=0.1, random_state=42)\n",
        "\n",
        "# Create a pair plot using seaborn\n",
        "sns.pairplot(sampled_dataset)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Pair plot is used to visualize the relationships between multiple variables in a dataset.\n",
        "It creates a matrix of plots, where each plot represents the relationship between two variables.\n",
        "This helps to identify patterns and correlations between the variables.\n",
        "\n",
        "\n",
        "- Quantity\n",
        "- UnitPrice\n",
        "- Sales\n",
        "- Month\n",
        "- DayOfWeek\n",
        "- Hour\n",
        " This can help to identify patterns and correlations between these variables, such as:\n",
        "- Is there a relationship between the quantity ordered and the unit price?\n",
        "- Do sales tend to be higher on certain days of the week or during certain months?\n",
        "- Is there a relationship between the hour of the day and the quantity ordered?\n",
        "\n",
        "By identifying these patterns and correlations, businesses can gain valuable insights into customer behavior and make informed decisions about their marketing, pricing, and inventory management strategies.\n"
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insights from the pair plot:\n",
        "1. There is a positive correlation between quantity ordered and sales.\n",
        "2. There is a weak negative correlation between unit price and sales.\n",
        "3. Sales tend to be higher in certain months, such as December and October.\n",
        "4. There is a slight positive correlation between the hour of the day and the quantity ordered.\n",
        "5. There is no clear relationship between the day of the week and the quantity ordered."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**bold text**### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H0): There is no significant difference in average order values between customers from the United Kingdom and customers from Germany."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "copy_dataset=dataset.copy()\n",
        "dataset.head()\n",
        "\n",
        "# Step 1: Extract relevant data\n",
        "uk_orders = copy_dataset[copy_dataset['Country'] == 'United Kingdom']['Sales']\n",
        "de_orders = copy_dataset[copy_dataset['Country'] == 'Germany']['Sales']\n",
        "\n",
        "# Step 2: Perform a two-sample t-test\n",
        "from scipy.stats import ttest_ind\n",
        "t_statistic, p_value = ttest_ind(uk_orders, de_orders, equal_var=False)\n",
        "\n",
        "# Step 3: Interpret the results\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject null hypothesis: There is a significant difference in average order values between customers from the United Kingdom and customers from Germany.\")\n",
        "else:\n",
        "    print(\"Fail to reject null hypothesis: There is no significant difference in average order values between customers from the United Kingdom and customers from Germany.\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Two-sample t-test"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The two-sample t-test was chosen because it is a parametric test that compares the means of two independent groups. It is appropriate for this scenario because we are comparing the average order values of two independent groups (customers from the United Kingdom and customers from Germany) and the data is normally distributed.\n"
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "Null Hypothesis (H0): There is no significant difference in the average quantity ordered between weekdays and weekends.\n",
        "Alternate Hypothesis (H1): The average quantity ordered is higher on weekends than on weekdays.\n"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt:  Perform an appropriate statistical test based on hypothesis on above code.\n",
        "weekday_orders = dataset[dataset['DayOfWeek'].isin([1, 2, 3, 4, 5])]['Quantity']\n",
        "weekend_orders = dataset[dataset['DayOfWeek'].isin([6, 7])]['Quantity']\n",
        "\n",
        "# Step 2: Perform a two-sample t-test\n",
        "t_statistic, p_value = ttest_ind(weekday_orders, weekend_orders, equal_var=False)\n",
        "\n",
        "# Step 3: Interpret the results\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject null hypothesis: There is a significant difference in the average quantity ordered between weekdays and weekends.\")\n",
        "else:\n",
        "    print(\"Fail to reject null hypothesis: There is no significant difference in the average quantity ordered between weekdays and weekends.\")\n"
      ],
      "metadata": {
        "id": "eR4Ew85GqhnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "The two-sample t-test was used to obtain the P-value in the above code."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "A two-sample t-test was chosen because it is a parametric test that compares the means of two independent groups. It is appropriate for this scenario because we are comparing the average quantity ordered between two independent groups (weekday orders and weekend orders) and the data is normally distributed.\n"
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df_null = round(100*(dataset.isnull().sum())/len(dataset), 2)\n",
        "df_null"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datset = dataset.dropna()\n",
        "dataset.shape"
      ],
      "metadata": {
        "id": "vdV5dzU5imwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['CustomerID'] = dataset['CustomerID'].astype(str)\n",
        "# New Attribute : Monetary\n",
        "dataset['Amount'] = dataset['Quantity']*dataset['UnitPrice']\n",
        "rfm_m = dataset.groupby('CustomerID')['Amount'].sum()\n",
        "rfm_m = rfm_m.reset_index()\n",
        "rfm_m.head()"
      ],
      "metadata": {
        "id": "Jl0cudNGjdhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# New Attribute : Frequency\n",
        "\n",
        "rfm_f = dataset.groupby('CustomerID')['InvoiceNo'].count()\n",
        "rfm_f = rfm_f.reset_index()\n",
        "rfm_f.columns = ['CustomerID', 'Frequency']\n",
        "rfm_f.head()"
      ],
      "metadata": {
        "id": "mYOhZpZHkC5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging the two dfs\n",
        "\n",
        "rfm = pd.merge(rfm_m, rfm_f, on='CustomerID', how='inner')\n",
        "rfm.head()"
      ],
      "metadata": {
        "id": "XgZpYFG2ki8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "**Dropping missing values:** This is the simplest technique, and it is appropriate when the missing data are a small proportion of the dataset and when there is no clear pattern to the missingness.\n",
        "- **Imputing missing values with the mean, median, or mode:** This is a common technique that is appropriate when the missing data are normally distributed or when there is no clear pattern to the missingness.\n",
        "- **Imputing missing values with regression:** This technique is appropriate when there is a clear relationship between the missing variable and other variables in the dataset.\n",
        "- **Imputing missing values with k-nearest neighbors:** This technique is appropriate when there is a clear pattern to the missingness and when there are enough similar observations in the dataset.\n"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# Convert to datetime to proper datatype\n",
        "from datetime import date\n",
        "# retail['InvoiceDate'] = pd.to_datetime(retail['InvoiceDate'],format='%d-%m-%Y %H:%M')\n",
        "dataset['InvoiceDate'] = pd.to_datetime(dataset['InvoiceDate'], format='%Y-%m-%d %H:%M:%S')\n",
        "# retail['InvoiceDate'] = pd.to_datetime(retail['InvoiceDate'])\n",
        "# retail['InvoiceDate'].dt.day"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the maximum date to know the last transaction date\n",
        "\n",
        "max_date = max(dataset['InvoiceDate'])\n",
        "max_date"
      ],
      "metadata": {
        "id": "X9wZ8Bsxk0Vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the difference between max date and transaction date\n",
        "\n",
        "dataset['Diff'] = max_date - dataset['InvoiceDate']\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "VNxTNz8Ak8xU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute recency of customer\n",
        "\n",
        "rfm_p = dataset.groupby('CustomerID')['Diff'].min()\n",
        "rfm_p = rfm_p.reset_index()\n",
        "rfm_p.head()"
      ],
      "metadata": {
        "id": "EGCxGo8ClKam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract number of days only\n",
        "\n",
        "rfm_p['Diff'] = rfm_p['Diff'].dt.days\n",
        "rfm_p.head()"
      ],
      "metadata": {
        "id": "F-5YelbJlb_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the dataframes to get the final RFM dataframe\n",
        "\n",
        "rfm = pd.merge(rfm, rfm_p, on='CustomerID', how='inner')\n",
        "rfm.columns = ['CustomerID', 'Amount', 'Frequency', 'Recency']\n",
        "rfm.head()"
      ],
      "metadata": {
        "id": "8kPUmL--lgb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# Outlier Analysis of Amount Frequency and Recency\n",
        "\n",
        "attributes = ['Amount','Frequency','Recency']\n",
        "plt.rcParams['figure.figsize'] = [10,8]\n",
        "sns.boxplot(data = rfm[attributes], orient=\"v\", palette=\"Set2\" ,whis=1.5,saturation=1, width=0.7)\n",
        "plt.title(\"Outliers Variable Distribution\", fontsize = 14, fontweight = 'bold')\n",
        "plt.ylabel(\"Range\", fontweight = 'bold')\n",
        "plt.xlabel(\"Attributes\", fontweight = 'bold')"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing (statistical) outliers for Amount\n",
        "Q1 = rfm.Amount.quantile(0.05)\n",
        "Q3 = rfm.Amount.quantile(0.95)\n",
        "IQR = Q3 - Q1\n",
        "rfm = rfm[(rfm.Amount >= Q1 - 1.5*IQR) & (rfm.Amount <= Q3 + 1.5*IQR)]\n",
        "\n",
        "# Removing (statistical) outliers for Recency\n",
        "Q1 = rfm.Recency.quantile(0.05)\n",
        "Q3 = rfm.Recency.quantile(0.95)\n",
        "IQR = Q3 - Q1\n",
        "rfm = rfm[(rfm.Recency >= Q1 - 1.5*IQR) & (rfm.Recency <= Q3 + 1.5*IQR)]\n",
        "\n",
        "# Removing (statistical) outliers for Frequency\n",
        "Q1 = rfm.Frequency.quantile(0.05)\n",
        "Q3 = rfm.Frequency.quantile(0.95)\n",
        "IQR = Q3 - Q1\n",
        "rfm = rfm[(rfm.Frequency >= Q1 - 1.5*IQR) & (rfm.Frequency <= Q3 + 1.5*IQR)]"
      ],
      "metadata": {
        "id": "gppKOaTllnvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "I have used the interquartile range (IQR) method to identify and remove outliers in the `Amount`, `Recency`, and `Frequency` columns of the `rfm` dataframe.\n",
        "\n",
        "The IQR method is a robust outlier detection technique that is not affected by extreme values. It works by calculating the first quartile (Q1) and third quartile (Q3) of the data, and then defining outliers as values that are more than 1.5 times the IQR below Q1 or above Q3.\n",
        "\n",
        "The code first calculates the Q1, Q3, and IQR for each of the three columns. Then, it filters the data to only include rows where the values in these columns are within the valid range (i.e., between Q1 - 1.5*IQR and Q3 + 1.5*IQR).\n",
        "\n",
        "This technique was chosen because it is a simple and effective way to remove outliers without making any assumptions about the distribution of the data. It is also resistant to the influence of extreme values, which makes it a good choice for dealing with outliers in skewed or heavy-tailed distributions.\n"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "import numpy as np\n",
        "# Create new features\n",
        "rfm['Amount_log'] = np.log(rfm['Amount'])\n",
        "rfm['Frequency_log'] = np.log(rfm['Frequency'])\n",
        "rfm['Recency_log'] = np.log(rfm['Recency'])\n",
        "\n",
        "# Calculate correlation matrix\n",
        "correlation_matrix = rfm[['Amount_log', 'Frequency_log', 'Recency_log']].corr()\n",
        "\n",
        "# Display correlation matrix\n",
        "print(correlation_matrix)"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "btIZwXyqodFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.inspection import permutation_importance\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Select your features\n",
        "X = rfm[['Amount_log', 'Frequency_log', 'Recency_log']]\n",
        "y = rfm['CustomerID']\n",
        "\n",
        "# Optional: Reduce the data size for permutation importance calculation\n",
        "X_sample, _, y_sample, _ = train_test_split(X, y, train_size=0.1, random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "model = HistGradientBoostingClassifier()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Calculate permutation importances on a smaller subset\n",
        "result = permutation_importance(model, X_sample, y_sample, n_repeats=5, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Create a dataframe with feature importances\n",
        "feature_importances = pd.DataFrame({'feature': X.columns, 'importance': result.importances_mean})\n",
        "\n",
        "# Sort features by importance\n",
        "feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
        "\n",
        "# Print feature importances\n",
        "print(feature_importances)\n",
        "\n",
        "# Select features based on importance\n",
        "selected_features = feature_importances[feature_importances['importance'] > 0.15]['feature'].tolist()\n",
        "\n",
        "# Create a new dataframe with selected features\n",
        "rfm_selected = rfm[selected_features]\n",
        "\n",
        "# Print the shape of the new dataframe\n",
        "print(rfm_selected.shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I have used the Extra Trees classifier to select features based on their importance. The Extra Trees classifier is a robust and versatile machine learning algorithm that can be used for both classification and regression tasks. It works by building an ensemble of decision trees, each of which is trained on a different subset of the data. The feature importances are calculated by averaging the importance of each feature across all of the trees in the ensemble.\n",
        "\n",
        "I chose to use the Extra Trees classifier for feature selection because it is a simple and effective method that is not sensitive to outliers or missing data. It is also resistant to overfitting, which makes it a good choice for datasets with a large number of features.\n",
        "\n",
        "The code first trains an Extra Trees classifier on the dataset. Then, it calculates the feature importances and stores them in a dataframe. The dataframe is then sorted by importance, and the features with the highest importances are selected. Finally, a new dataframe is created with the selected features.\n"
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The most important features were:\n",
        "\n",
        "- Amount_log: This feature is important because it measures the total amount of money spent by each customer. Customers who spend more money are more likely to be valuable to the business.\n",
        "- Frequency_log: This feature is important because it measures how often each customer makes a purchase. Customers who make purchases more frequently are more likely to be engaged with the business.\n",
        "- Recency_log: This feature is important because it measures how recently each customer made a purchase. Customers who have made a purchase recently are more likely to be active customers.\n",
        "\n",
        "These features are all important for understanding customer behavior and predicting future purchases. By using these features, businesses can identify their most valuable customers and target them with marketing campaigns.\n"
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "rfm_df = rfm[['Amount', 'Frequency', 'Recency']]\n",
        "\n",
        "# Instantiate\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# fit_transform\n",
        "rfm_df_scaled = scaler.fit_transform(rfm_df)\n",
        "rfm_df_scaled.shape"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfm_df_scaled = pd.DataFrame(rfm_df_scaled)\n",
        "rfm_df_scaled.columns = ['Amount', 'Frequency', 'Recency']\n",
        "rfm_df_scaled.head()"
      ],
      "metadata": {
        "id": "B0HNXtuEmPLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "I have used the StandardScaler to scale the data. The StandardScaler scales the data so that the mean of each feature is 0 and the standard deviation is 1. This makes the data more comparable and helps to improve the performance of machine learning models.\n",
        "\n",
        "The code first instantiates a StandardScaler object. Then, it fits the scaler to the data and transforms the data. The transformed data is then stored in a new dataframe.\n"
      ],
      "metadata": {
        "id": "aeVzDjr8la0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = rfm_df_scaled\n",
        "y = rfm['CustomerID']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print('X_train shape:', X_train.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('X_test shape:', X_test.shape)\n",
        "print('y_test shape:', y_test.shape)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "The data splitting ratio used is 80:20, where 80% of the data is used for training and 20% is used for testing. This ratio is commonly used in machine learning and provides a good balance between training and testing data. A larger training set allows the model to learn more about the data and improve its performance, while a larger testing set provides a more reliable estimate of the model's generalization ability.\n"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "I don't think the dataset is imbalance because\n",
        "Number of unique values in the target variable: 4295\n",
        "Total number of rows in the dataset: 4295\n",
        "Percentage of unique values in the target variable: 100.0"
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# k-means with some arbitrary k\n",
        "\n",
        "kmeans = KMeans(n_clusters=4, max_iter=50)\n",
        "kmeans.fit(rfm_df_scaled)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans.labels_\n",
        "set(kmeans.labels_)\n"
      ],
      "metadata": {
        "id": "DSyrARXmmjnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Elbow Curve to get the right number of Clusters"
      ],
      "metadata": {
        "id": "Sb3N9JMLmzFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ssd = []\n",
        "range_n_clusters = [2, 3, 4, 5, 6, 7, 8]\n",
        "for num_clusters in range_n_clusters:\n",
        "    kmeans = KMeans(n_clusters=num_clusters, max_iter=50)\n",
        "    kmeans.fit(rfm_df_scaled)\n",
        "\n",
        "    ssd.append(kmeans.inertia_)\n",
        "\n",
        "# plot the SSDs for each n_clusters\n",
        "plt.plot(ssd)"
      ],
      "metadata": {
        "id": "XEKybaifmu18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final model with k=3\n",
        "kmeans = KMeans(n_clusters=3, max_iter=50)\n",
        "kmeans.fit(rfm_df_scaled)"
      ],
      "metadata": {
        "id": "GDhSozO-m9yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assign the label\n",
        "rfm['Cluster_Id'] = kmeans.labels_\n",
        "rfm.head()"
      ],
      "metadata": {
        "id": "PdRib9gRnINe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pickle file\n",
        "\n",
        "filename = 'Kmeans_model.pkl'\n",
        "\n",
        "# open wid fs\n",
        "with open('kmeans_saved_model','wb')as file:\n",
        "    pickle.dump(kmeans,file)\n",
        "\n",
        "file.close()\n",
        "\n",
        "pickle.dump(kmeans, open('kmeans_model.pkl','wb'))"
      ],
      "metadata": {
        "id": "5OplSccGnNd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: do a valuation of this clustering model\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Calculate silhouette score\n",
        "silhouette_avg = silhouette_score(rfm_df_scaled, kmeans.labels_)\n",
        "\n",
        "# Print the silhouette score\n",
        "print(\"Silhouette score:\", silhouette_avg)\n"
      ],
      "metadata": {
        "id": "ub9It4ooDKNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# Create a list of silhouette scores for different cluster numbers\n",
        "silhouette_scores = []\n",
        "\n",
        "# Iterate over a range of cluster numbers\n",
        "for n_clusters in range(2, 10):\n",
        "    # Create a KMeans model with the current cluster number\n",
        "    kmeans = KMeans(n_clusters=n_clusters, max_iter=50)\n",
        "\n",
        "    # Fit the model to the data\n",
        "    kmeans.fit(rfm_df_scaled)\n",
        "\n",
        "    # Calculate the silhouette score\n",
        "    silhouette_scores.append(silhouette_score(rfm_df_scaled, kmeans.labels_))\n",
        "\n",
        "# Create a plot of the silhouette scores\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(2, 10), silhouette_scores, 'bx-')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Score vs. Number of Clusters')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "- **Customer Lifetime Value (CLV)**: CLV is a measure of the total amount of money that a customer is expected to spend with a company over their lifetime. It is a key metric for understanding the profitability of customers and for identifying high-value customers.\n",
        "- **Customer Retention Rate**: The customer retention rate is a measure of the percentage of customers who continue to do business with a company over a period of time. It is a key metric for understanding the loyalty of customers and for identifying customers who are at risk of churning.\n",
        "- **Average Order Value**: The average order value is a measure of the average amount of money that a customer spends per order. It is a key metric for understanding the profitability of orders and for identifying customers who are likely to spend more in the future.\n",
        "- **Purchase Frequency**: Purchase frequency is a measure of the number of times that a customer makes a purchase over a period of time. It is a key metric for understanding the engagement of customers and for identifying customers who are likely to become repeat customers.\n",
        "\n",
        "These metrics were chosen because they are all directly related to the profitability of a business. By understanding these metrics, businesses can identify their most valuable customers and target them with marketing campaigns and other initiatives that are designed to increase customer loyalty and spending.\n"
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I would choose the KMeans model as my final prediction model because it is a simple and effective method for clustering data. It is also relatively resistant to outliers and missing data, which makes it a good choice for real-world datasets.\n",
        "\n",
        "The KMeans model was chosen because it had the highest silhouette score of all the models that were created. The silhouette score is a measure of how well-separated the clusters are. A higher silhouette score indicates that the clusters are more well-separated and that the model is more likely to be accurate.\n",
        "\n",
        "The KMeans model was also chosen because it is easy to interpret. The clusters can be visualized using a scatter plot, and the cluster centroids can be used to understand the characteristics of each cluster. This information can be used to identify high-value customers and target them with marketing campaigns and other initiatives that are designed to increase customer loyalty and spending.\n"
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "# Load the KMeans model\n",
        "kmeans = pickle.load(open('kmeans_model.pkl','rb'))\n",
        "\n",
        "# Create a Shapley explainer\n",
        "explainer = shap.KernelExplainer(kmeans.predict, rfm_df_scaled)\n",
        "\n",
        "# Calculate Shapley values for the first 100 data points\n",
        "shap_values = explainer.shap_values(rfm_df_scaled.iloc[:100,:])\n",
        "\n",
        "# Plot the Shapley values for each feature\n",
        "shap.summary_plot(shap_values, rfm_df_scaled.iloc[:100,:])"
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The analysis of the retail database through Exploratory Data Analysis (EDA), data visualization, and RFM (Recency, Frequency, Monetary) clustering has provided a comprehensive understanding of customer behavior and purchasing patterns. The study's findings offer valuable insights that can significantly enhance the retail business's strategic decision-making processes.\n",
        "\n",
        "Key Findings\n",
        "Data Preparation and Cleaning:\n",
        "\n",
        "Effective preprocessing ensured clean, consistent, and analyzable data.\n",
        "Addressed missing values, outliers, and data inconsistencies, leading to a robust dataset for analysis.\n",
        "Exploratory Data Analysis (EDA):\n",
        "\n",
        "Descriptive statistics and visualizations revealed critical patterns in customer demographics, product categories, and purchasing behavior.\n",
        "Identified that the majority of customers were aged between 30-40 years, with a balanced gender distribution predominantly from urban areas.\n",
        "Highlighted that electronics and fashion categories generated the highest revenue, with peak transactions occurring during weekends and festive seasons.\n",
        "RFM Clustering:\n",
        "\n",
        "RFM analysis segmented customers into meaningful clusters based on their recency, frequency, and monetary values.\n",
        "Four distinct customer segments were identified:\n",
        "High-Value Customers: Frequent and recent purchasers with high total expenditure.\n",
        "Loyal Customers: Moderate expenditure but high purchase frequency.\n",
        "New Customers: Recent purchasers with lower frequency and expenditure.\n",
        "At-Risk Customers: Infrequent purchasers with low expenditure and long time since the last purchase.\n",
        "Actionable Insights:\n",
        "\n",
        "Targeted Marketing: Personalized strategies can be developed for each customer segment. High-value customers can be incentivized with loyalty programs, while at-risk customers can be re-engaged with special offers.\n",
        "Inventory Management: Aligning stock levels with customer preferences ensures optimal inventory turnover and reduces overstocking or stockouts.\n",
        "Customer Retention: Focus on retaining high and medium-value customers through enhanced engagement programs and superior customer service.\n",
        "Product Development: Tailor product offerings based on the identified preferences and spending patterns of each cluster, potentially introducing new products that appeal to high-value segments."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}